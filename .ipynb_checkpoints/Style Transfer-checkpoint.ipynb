{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import an image into the environment and reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_image = Image.open(\"content_image.jpeg\").resize((512,512))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import the style image into the environment and reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_image = Image.open(\"style_image.jpg\").resize((512,512))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert the images to numpy arrays for further processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_array = np.asarray(content_image, \"float32\")\n",
    "content_array = np.expand_dims(content_array, axis = 0)\n",
    "\n",
    "style_array = np.asarray(style_image, \"float32\")\n",
    "style_array = np.expand_dims(style_array, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 512, 512, 3), (1, 512, 512, 3))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_array.shape, style_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We added one extra dimension using the expand_dims function for concatenating these images into a tensor later, which will be the input to a VGG conv net model.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now according to the paper written for VGG we subtract the input images with the mean of the data which was calculated in the imagenet competition. This is available in google. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We do this to center the intensities of the image at zero to improve the accuracy and training speed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_array[:,:,:,0] -= 103.939\n",
    "content_array[:,:,:,1] -= 116.779\n",
    "content_array[:,:,:,2] -= 123.68\n",
    "\n",
    "style_array[:,:,:,0] -= 103.939\n",
    "style_array[:,:,:,1] -= 116.779\n",
    "style_array[:,:,:,2] -= 123.68\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We convert our RGB values to BGR to match the architecture of VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_array = content_array[:,:,:, ::-1]\n",
    "style_array = style_array[:,:,:, ::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now lets create our input tensor using keras backend(tensorflow graph). what is backend is explained really well here. check it out. http://keras.io/backend/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_variable = backend.variable(content_array)\n",
    "style_variable = backend.variable(style_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's initialize a combination image variable using the backend placeholder function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "combination_variable = backend.placeholder(shape = content_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([1, 512, 512, 3]),\n",
       " TensorShape([1, 512, 512, 3]),\n",
       " TensorShape([1, 512, 512, 3]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combination_variable.shape, style_variable.shape, content_variable.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the concatenate function in backend we will concatenate these three variables which will return a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_variable = backend.concatenate([ content_variable, style_variable, combination_variable], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'concat:0' shape=(3, 512, 512, 3) dtype=float32>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notice how we are concatenating on the first dimension? This way we preserve all the data we have so far. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As mentioned above we will be using a pre trained model called VGG16 which was initally developed as a classification problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If you are not familiar with conv nets i will write another blog post explaining in detail what they mean and how they are useful. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VGG 16 is a deep learning model with 16 layers and is a conv net classifier. Conv nets are believed to be able to percieve and \"understand\" images. But every conv net model have a few layers which perform a classification these are called fully connected layers or FC layers. We dont need these layers for our algorithm so we will choose only the ones we will need.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can import the VGG16 model from keras applications "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_model = VGG16(input_tensor = tensor_variable, weights = \"imagenet\", include_top = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### include_top = False will ignore the fully connected layers and only retrieve the conv net layers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.input_layer.InputLayer at 0x7f18fc1b3bd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f18fc641950>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f18fc1e4e10>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x7f18fc1b3d90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f18fc190c50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f18f82cad10>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x7f18f82d1a50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f18f82d1b10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f18f82d5990>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f18f82dac90>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x7f18f82e0d10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f18f82e0d90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f18f82e7fd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f18f826dd10>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x7f18f8273fd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f18f8273f50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f18f8279ed0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f18f82846d0>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x7f18f827fb90>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg_model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_1': <tf.Tensor 'concat:0' shape=(3, 512, 512, 3) dtype=float32>,\n",
       " 'block1_conv1': <tf.Tensor 'block1_conv1/Relu:0' shape=(3, 512, 512, 64) dtype=float32>,\n",
       " 'block1_conv2': <tf.Tensor 'block1_conv2/Relu:0' shape=(3, 512, 512, 64) dtype=float32>,\n",
       " 'block1_pool': <tf.Tensor 'block1_pool/MaxPool:0' shape=(3, 256, 256, 64) dtype=float32>,\n",
       " 'block2_conv1': <tf.Tensor 'block2_conv1/Relu:0' shape=(3, 256, 256, 128) dtype=float32>,\n",
       " 'block2_conv2': <tf.Tensor 'block2_conv2/Relu:0' shape=(3, 256, 256, 128) dtype=float32>,\n",
       " 'block2_pool': <tf.Tensor 'block2_pool/MaxPool:0' shape=(3, 128, 128, 128) dtype=float32>,\n",
       " 'block3_conv1': <tf.Tensor 'block3_conv1/Relu:0' shape=(3, 128, 128, 256) dtype=float32>,\n",
       " 'block3_conv2': <tf.Tensor 'block3_conv2/Relu:0' shape=(3, 128, 128, 256) dtype=float32>,\n",
       " 'block3_conv3': <tf.Tensor 'block3_conv3/Relu:0' shape=(3, 128, 128, 256) dtype=float32>,\n",
       " 'block3_pool': <tf.Tensor 'block3_pool/MaxPool:0' shape=(3, 64, 64, 256) dtype=float32>,\n",
       " 'block4_conv1': <tf.Tensor 'block4_conv1/Relu:0' shape=(3, 64, 64, 512) dtype=float32>,\n",
       " 'block4_conv2': <tf.Tensor 'block4_conv2/Relu:0' shape=(3, 64, 64, 512) dtype=float32>,\n",
       " 'block4_conv3': <tf.Tensor 'block4_conv3/Relu:0' shape=(3, 64, 64, 512) dtype=float32>,\n",
       " 'block4_pool': <tf.Tensor 'block4_pool/MaxPool:0' shape=(3, 32, 32, 512) dtype=float32>,\n",
       " 'block5_conv1': <tf.Tensor 'block5_conv1/Relu:0' shape=(3, 32, 32, 512) dtype=float32>,\n",
       " 'block5_conv2': <tf.Tensor 'block5_conv2/Relu:0' shape=(3, 32, 32, 512) dtype=float32>,\n",
       " 'block5_conv3': <tf.Tensor 'block5_conv3/Relu:0' shape=(3, 32, 32, 512) dtype=float32>,\n",
       " 'block5_pool': <tf.Tensor 'block5_pool/MaxPool:0' shape=(3, 16, 16, 512) dtype=float32>}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers  = dict([(layer.name, layer.output) for layer in vgg_model.layers])\n",
    "layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our model and our input is ready. We need to work on how to calculate loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since the combination_image is a combination of both content and style images we need to calculate the loss respective to both these images. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We choose the numbers based on the influence we want from each of these images in the combined image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_weight = 0.025\n",
    "style_weight = 5\n",
    "total_variation_weight = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing a tensor with 0 to store the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = backend.variable(0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcuate loss between the combination image and the content image using the euclidean loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing the block2_conv2 layer for extracting  the content image features but we can choose others as well. \n",
    "Based on tensor we created before we can extract each of the features using the layers dict which contains block2_conv2 tensor.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"block2_conv2/Relu:0\", shape=(3, 256, 256, 128), dtype=float32)\n",
      "Tensor(\"strided_slice_10:0\", shape=(256, 256, 128), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def calculate_content_loss(content, combination):\n",
    "    return backend.sum(backend.square(content - combination))\n",
    "    \n",
    "layer_features = layers[\"block2_conv2\"]\n",
    "print(layer_features)\n",
    "content_image_features = layer_features[0,:,:,:]\n",
    "combination_image_features = layer_features[2,:,:,:]\n",
    "print(content_image_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finally calculating loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=() dtype=float32, numpy=0.0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.assign_add(content_weight * calculate_content_loss(content_image_features, combination_image_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Style Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating style loss is a bit tricky but we will get through this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the gram matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gram_matrix(matrix):\n",
    "    features = backend.batch_flatten(backend.permute_dimensions(matrix, (2, 0, 1)))\n",
    "    return backend.dot(backend.transpose(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_style_loss(style, combination):\n",
    "    style_gram = calculate_gram_matrix(style)\n",
    "    combination_gram = calculate_gram_matrix(combination)\n",
    "    channels = 3\n",
    "    size = 512 * 512 ## height * width\n",
    "    return backend.sum(backend.square(style_gram - combination_gram)) / (4. * (channels **2) * (size ** 2))1\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
